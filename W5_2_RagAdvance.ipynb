{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dajinfx/hanghaeAI2/blob/main/W5_2_RagAdvance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##과제: 다양한 형태의 입력을 가지는 LLM 서비스 개발\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dajinfx/hanghaeAI2/blob/main/W5_2_RagAdvance.ipynb)"
      ],
      "metadata": {
        "id": "TXoMt2a54_5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community langchain-chroma langchain-openai bs4"
      ],
      "metadata": {
        "id": "VuMcSR4sYBcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n"
      ],
      "metadata": {
        "id": "AZ7vwOOPnIFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "t-VO-M06YQ-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "id": "QyDSknUdg4JB",
        "outputId": "cdedc446-f7b7-453a-b4be-3efa1bad9cc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m256.0/298.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unstructured"
      ],
      "metadata": {
        "id": "B2NzKLF0kHXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"unstructured[pdf]\""
      ],
      "metadata": {
        "id": "65eDPKcykWHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model=\"gpt-4o\"\n",
        "temperature = 0.7  # 각 token을 샘플링할 때 사용하는 temperature 값입니다.\n",
        "max_tokens = 4096  # 생성하는 최대 token 개수 입니다.\n",
        "frequency_penalty = 0.0  # 같은 단어가 반복적으로 나오는 것을 방지하기 위한 옵션입니다."
      ],
      "metadata": {
        "id": "h-Vl4tf3YUgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[MyCode] Upload apikey to googlelab"
      ],
      "metadata": {
        "id": "UvoUNzPngleI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    print(f\" {filename} is uploaded\")\n",
        "\n",
        "with open('OpenAI_APIKey.txt', 'r') as file:\n",
        "    api_key = file.read()"
      ],
      "metadata": {
        "id": "MzgZzPdYn1Yo",
        "outputId": "ad585757-7f76-4453-b8ab-372c6b40a589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-76b902d8-14aa-44f0-86b0-9c99474418af\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-76b902d8-14aa-44f0-86b0-9c99474418af\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving OpenAI_APIKey.txt to OpenAI_APIKey.txt\n",
            " OpenAI_APIKey.txt is uploaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "#from langchain.document_loaders import UnstructuredFileLoader"
      ],
      "metadata": {
        "id": "2nppbkZrghUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_url = \"https://arxiv.org/pdf/2005.11401\""
      ],
      "metadata": {
        "id": "fwlqtqBIfT0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "SerZ3QPfmR_n",
        "outputId": "e241db92-f770-4cea-fccb-b38a8befcafe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[MyCode] UnstructuredURLLoader\n",
        "\n",
        "UnstructuredURLLoader 로서 PDF 업로드 시도 실패했습니다,  원인 불명, 파악필요."
      ],
      "metadata": {
        "id": "Sb1pThBIlEoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # 修复路径\n",
        "from langchain.schema import Document\n",
        "\n",
        "# 使用 UnstructuredURLLoader 加载 PDF 文件\n",
        "loader = UnstructuredURLLoader(urls=[file_url])\n",
        "docs = loader.load()\n",
        "'''"
      ],
      "metadata": {
        "id": "uV55Jf8rgG_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c0ad93f-fbcb-47da-f9d3-7fb19affad69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:langchain_community.document_loaders.url:Error fetching or processing https://arxiv.org/pdf/2005.11401, exception: Unable to get page count. Is poppler installed and in PATH?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[MyCode] PyPDFLoader\n",
        "\n",
        "PyPDFLoader 이용으로  업로드성공"
      ],
      "metadata": {
        "id": "hlCXQ3h9lgvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\n",
        "    file_path= file_url,\n",
        ")\n",
        "docs = loader.load()\n",
        "print('논문 페이지 수:', len(docs))"
      ],
      "metadata": {
        "id": "bQVrx_lKgEkB",
        "outputId": "4bd36e64-b1b0-4a7a-8a8e-07c4e58d3c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "논문 페이지 수: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "8MIn4UgDhCX3",
        "outputId": "ef56ac9d-ffa1-48d9-b932-b935c8f4e2cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://arxiv.org/pdf/2005.11401', 'page': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印加载的文档\n",
        "for doc in docs:\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "id": "hB1OnNFpnYQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq8tBAEdm_SE"
      },
      "outputs": [],
      "source": [
        "# 텍스트 분할기 설정\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=3000,  # 각 청크의 최대 길이\n",
        "    chunk_overlap=500  # 청크 간 겹침 부분 최소화\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chroma, Embedding\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=OpenAIEmbeddings(api_key=api_key)\n",
        ")"
      ],
      "metadata": {
        "id": "2im_sSUBmWXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(splits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UdpCXP6mvGI",
        "outputId": "0c0baaa2-4084-4a9e-9132-d046ece8d5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LangChain Map-Reduce"
      ],
      "metadata": {
        "id": "43siUWJDgUsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map에 사용할 prompt template\n",
        "map_template = \"\"\"다음은 문서 중 일부 내용입니다\n",
        "{pages}\n",
        "이 문서 목록을 기반으로 주요 내용을 요약해 주세요.\n",
        "\n",
        "답변: \"\"\"\n",
        "\n",
        "# Map 프롬프트\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "\n",
        "# Map에서 수행할 LLMChain 정의\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model_name=gpt_model,\n",
        "                 api_key=api_key)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ],
      "metadata": {
        "id": "YZLzMcUunJpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce 단계에서 처리할 프롬프트 정의\n",
        "reduce_template = \"\"\"다음은 요약의 집합입니다:\n",
        "{doc_summaries}\n",
        "이것들을 바탕으로 통합된 요약을 만들어 주세요.\n",
        "요약을 할때,\n",
        "1)***\n",
        "2)***\n",
        "3)***\n",
        "이렇게 매개 요약마다 번호를 매겨주세요.\n",
        "답변:\"\"\"\n",
        "\n",
        "# Reduce 프롬프트 완성\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "\n",
        "# Reduce에서 수행할 LLMChain 정의\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
      ],
      "metadata": {
        "id": "13ACNWu0oLJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains import ReduceDocumentsChain\n",
        "\n",
        "# 문서의 목록을 받아들여, 이를 단일 문자열로 결합하고, 이를 LLMChain에 전달합니다.\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain,\n",
        "    document_variable_name=\"doc_summaries\" # Reduce 프롬프트에 대입되는 변수\n",
        ")\n",
        "\n",
        "# Map 문서를 통합하고 순차적으로 Reduce합니다.\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # 호출되는 최종 체인입니다.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # 문서가 `StuffDocumentsChain`의 컨텍스트를 초과하는 경우\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # 문서를 그룹화할 때의 토큰 최대 개수입니다.\n",
        "    token_max=4000,\n",
        ")"
      ],
      "metadata": {
        "id": "LqAWme4ecIze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import MapReduceDocumentsChain\n",
        "\n",
        "# 문서들에 체인을 매핑하여 결합하고, 그 다음 결과들을 결합합니다.\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map 체인\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce 체인\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # 문서를 넣을 llm_chain의 변수 이름(map_template 에 정의된 변수명)\n",
        "    document_variable_name=\"pages\",\n",
        "    # 출력에서 매핑 단계의 결과를 반환합니다.\n",
        "    return_intermediate_steps=False,\n",
        ")"
      ],
      "metadata": {
        "id": "ozkOJnGrcTPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map-Reduce 체인 실행\n",
        "# 입력: 분할된 도큐먼트(②의 결과물)\n",
        "result = map_reduce_chain.run(splits)\n",
        "# 요약결과 출력\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6QZUKoMcfiH",
        "outputId": "44585eff-c9a3-4536-84b7-ced2fd40ac19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) ***RAG 모델의 성능과 특징***: RAG(회수-생성) 모델은 대규모 사전 학습된 언어 모델의 한계를 극복하기 위해 제안된 모델로, 파라메트릭 메모리와 비파라메트릭 메모리를 결합하여 지식 집약적인 작업에서 우수한 성능을 보입니다. RAG-Sequence와 RAG-Token의 두 가지 변형을 통해 다양한 NLP 작업에서 최첨단 성능을 달성하며, 특히 Jeopardy 질문 생성과 FEVER 사실 검증 작업에서 강력한 성능을 입증합니다. RAG 모델은 BART 모델에 비해 더 구체적이고 사실적으로 정확한 응답을 생성하며, 정보 검색과 생성 작업에서 더 효과적입니다.\n",
            "\n",
            "2) ***NLP 작업에서의 검색 기법 활용과 사회적 영향***: 정보 검색은 개방형 질문 응답, 사실 확인, 장문 질문 응답 등 다양한 NLP 작업의 성능을 향상시킬 수 있으며, RAG 모델은 이러한 검색 기법을 활용하여 성능을 극대화합니다. RAG 모델은 Wikipedia와 같은 사실적 지식에 기반하여 더 정확하고 해석 가능한 결과를 생성할 수 있으며, 다양한 분야에서 긍정적인 사회적 이점을 제공합니다. 그러나 외부 지식 소스의 편견과 부정확성으로 인한 위험도 존재하며, 이를 완화하기 위한 AI 시스템의 활용이 필요합니다.\n",
            "\n",
            "3) ***연구 동향과 모델 성능 비교***: NLP와 AI 분야의 최신 연구는 자연어 생성, 번역, 이해, 대화 시스템 등 다양한 응용 분야에서의 성능 향상과 문제 해결을 목표로 하고 있습니다. T5-11B 모델은 \"closed-book\" 오픈 도메인 QA 모델 중 가장 높은 성능을 보이며, RAG-Sequence 모델은 하이브리드 파라메트릭/비파라메트릭 접근을 통해 더 적은 파라미터로도 강력한 성능을 발휘합니다. 비파라메트릭 메모리 인덱스와 검색 컴포넌트의 문제점도 존재하지만, RAG 모델은 다양한 NLP 작업에서 기존의 최첨단 모델을 능가하는 성능을 보입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "\n",
        "요약이 잘 출력이 되는걸로 보입니다."
      ],
      "metadata": {
        "id": "hIRlbabLhN9L"
      }
    }
  ]
}